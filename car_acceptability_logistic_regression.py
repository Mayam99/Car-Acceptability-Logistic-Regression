# -*- coding: utf-8 -*-
"""Car Acceptability Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ACl68Gdc0Yd1HEf1ro1L8jQmI59Ykwv9

##Introduction to the Car Acceptability Classification Database
The Car Acceptability Classification Database is derived from a hierarchical decision model initially developed to demonstrate DEX, an expert system for decision-making. The original work by M. Bohanec and V. Rajkovic, titled "Expert System for Decision Making," was published in Sistemica 1(1), pp. 145-157, 1990. This dataset serves as a practical illustration of evaluating cars based on various criteria and is widely used for classification tasks in machine learning.

Background and Purpose
The primary purpose of this dataset is to evaluate cars according to a structured model that considers multiple attributes. The decision model simplifies complex evaluations by breaking them down into more manageable sub-decisions, following a hierarchical approach. This type of model is particularly useful in expert systems where decisions are made based on a set of rules derived from domain knowledge.

Dataset Description

The dataset comprises six features and one target variable:

1) Buying Price:

Represents the initial cost of the car.
Possible values: vhigh (very high), high, med (medium), low.

2) Maintenance Price:

Represents the cost associated with maintaining the car.
Possible values: vhigh, high, med, low.

3) Number of Doors:

Indicates the number of doors on the car.
Possible values: 2, 3, 4, 5more.

4) Person Capacity:

Represents the maximum number of persons the car can carry.
Possible values: 2, 4, more.

5) Size of Luggage Boot:

Describes the capacity of the luggage boot.
Possible values: small, med, big.

6) Safety:

Indicates the safety level of the car.
Possible values: low, med, high.

7) Car Acceptability (Target Variable):

Classifies the car based on the aforementioned features.
Possible values: unacc (unacceptable), acc (acceptable), good, vgood (very good).

Concept Structure and Evaluation Criteria

The evaluation model assesses cars using a decision tree-like structure where each node represents a decision criterion. The final acceptability of a car is determined by the combined assessment of all criteria. Hereâ€™s an overview of the concept structure:

1) Price Evaluation:

Comprises Buying Price and Maintenance Price.

2) Capacity Evaluation:

Includes Number of Doors and Person Capacity.

3) Usability and Safety:

Considers Size of Luggage Boot and Safety.

Each feature contributes to the overall decision, influencing the car's classification as unacc, acc, good, or vgood. The hierarchical model ensures that all relevant aspects are taken into account, providing a comprehensive evaluation.

Objective

The primary objective of analyzing this dataset is to build a predictive model that can classify the acceptability of cars based on the given features. For this project, we will utilize a logistic regression model due to its effectiveness in binary and multiclass classification problems.

Source:

https://www.kaggle.com/datasets/subhajeetdas/car-acceptability-classification-dataset/data
"""

import pandas as pd #Importing pandas for data manipulation and analysis.
from sklearn.model_selection import train_test_split #Importing train_test_split to split the data into training and testing sets.
from sklearn.preprocessing import OneHotEncoder #Importing OneHotEncoder for converting categorical features into numerical format.
from sklearn.linear_model import LogisticRegression #Importing LogisticRegression to build the logistic regression model for classification.
from sklearn.metrics import classification_report, accuracy_score #Importing classification_report and accuracy_score for model evaluation metrics.

df = pd.read_csv("car.csv") #Reading the dataset from a CSV file into a pandas DataFrame for analysis.

df.head() #Displaying the first few rows of the DataFrame to get an overview of the data.

df.columns #Listing the column names of the DataFrame to see the available features.

df.shape #Getting the number of rows and columns in the DataFrame to understand the dataset's size.

df.isnull().sum() #Checking for missing values in each column of the DataFrame.

df.info() #Displaying a concise summary of the DataFrame, including data types and non-null counts.

# Encode categorical features
encoder = OneHotEncoder(drop='first')  # drop='first' to avoid multicollinearity
encoded_features = encoder.fit_transform(df.drop('Car_Acceptability', axis=1))

"""### OneHotEncoder is used to convert categorical features into a binary format, making them suitable for machine learning algorithms.
### The parameter drop='first' is specified to avoid the dummy variable trap by excluding the first category of each feature.
### The fit_transform method fits the encoder to the data and transforms it, creating new binary columns for each category.


"""

# Split data
X_train, X_test, y_train, y_test = train_test_split(encoded_features, df['Car_Acceptability'], test_size=0.2, random_state=42)

"""### train_test_split is used to divide the dataset into training and testing sets.
### encoded_features contains the transformed features, and df['Car_Acceptability'] is the target variable.
### test_size=0.2 indicates that 20% of the data is reserved for testing, which helps evaluate the model's generalization ability.
### random_state=42 ensures reproducibility by setting a seed for the random number generator.

"""

# Build the model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

"""### LogisticRegression is chosen for its ability to model binary classification problems.
### max_iter=1000 increases the maximum number of iterations, allowing the solver more time to converge, which can be important for larger or more complex datasets.
### The fit method trains the model on the training data (X_train, y_train), learning the relationship between the features and the target variable.


"""

# Predictions
y_pred = model.predict(X_test)

"""### The predict method uses the trained Logistic Regression model to make predictions on the test data (X_test).
### y_pred contains the predicted labels, which will be compared against the actual labels (y_test) to assess the model's accuracy and other metrics.


"""

# Evaluate
print(classification_report(y_test, y_pred))
print('Accuracy:', accuracy_score(y_test, y_pred))

"""### classification_report provides a comprehensive overview of the model's performance, including metrics like precision, recall, and F1-score for each class.
### Precision measures the accuracy of positive predictions, recall measures the ability to find all positive instances, and F1-score balances precision and recall.
### The report helps in understanding the model's strengths and weaknesses, particularly in cases of class imbalance.
### accuracy_score calculates the overall accuracy, representing the proportion of correctly predicted instances out of the total.


"""

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

"""### accuracy_score calculates the proportion of correctly predicted instances out of the total instances in the test set.
### The result is multiplied by 100 to express the accuracy as a percentage, making it easier to interpret.
### This metric provides a general sense of how well the model is performing, but should be considered alongside other metrics like precision and recall, especially if the data is imbalanced.


"""